{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Welcome to the SETI Institute Code Challenge!\n",
    "\n",
    "This first tutorial will explain a little bit on what the data is and where to get it.\n",
    "\n",
    "# Update 21 June 2017\n",
    "\n",
    "We learned a lot at the hackathon on June 10-11th and decided to regenerate the primary data set. This is called the `v3` primary data set. The changes, compared to `v2` are: the noise background is gaussian white noise instead of noise from the Sun, the signal amplitudes are higher and the characteristics should make them more distinguishable, and there are only 140k in the full set (20k per signal type), compared with 350k previously (50k per signal type).\n",
    "\n",
    "The `basic` data set remains unchanged from before.\n",
    "\n",
    "# Introduction\n",
    "\n",
    "For the Code Challenge, you will be using the **\"primary\" data set**, as we've called it. The primary data set is   \n",
    "\n",
    "    * labeled data set of 140,000 simulated signals\n",
    "    * 7 different labels, or \"signal classifications\"\n",
    "    * total of about 51 GB of data\n",
    "    \n",
    "This data set should be used to train your models. \n",
    "\n",
    "**You do not need to use all the data to train your models** if you do not want to or need to consume the entire set. \n",
    "\n",
    "There are also a **`small` and a `medium` sized subset** of these primary data files. \n",
    "\n",
    "\n",
    "## Simple Data Format\n",
    "\n",
    "Each data file has a simple format: \n",
    "\n",
    "    * file name = <UUID>.dat\n",
    "    * a JSON header in the first line that contains:\n",
    "        * UUID\n",
    "        * signal_classification (label)\n",
    "    * followed by stream complex-valued time-series data. \n",
    "\n",
    "The `ibmseti` Python package is available to assist in reading this data and performing some basic operations for you. \n",
    "\n",
    "## Basic Warmup Data Set.\n",
    "\n",
    "There is also a second, simple and clean data set that you may use for warmup, which we call the **\"basic\" data set**. This basic set should be used as a sanity check and for very early-stage prototyping. We recommend that everybody starts with this. \n",
    "\n",
    "    * Only 4 different signal classifications\n",
    "    * 1000 simulation files for each class: 4000 files total\n",
    "    * Available as single zip file\n",
    "    * ~1 GB in total. \n",
    "       \n",
    "### Basic Set versus Primary Set\n",
    "\n",
    "> The difference between the `basic` and `primary` data sets is that the signals simulated in the `basic` set have, on average, much higher signal to noise ratio (they are larger amplitude signals). They also have other characteristics that will make the different signal classes very distinguishable. **You should be able to get very high signal classification accuracy with the basic data set.**  The primary data set has smaller amplitude signals and can look more similar to each other, making classification accuracy more difficult with this data set. There are also only 4 classes in the basic data set and 7 classes in the primary set. \n",
    "\n",
    "\n",
    "## Primary Data Sets\n",
    "\n",
    "### Primary Small\n",
    "\n",
    "The `primary small` is a subset of the full primary data set.  Use for early-stage prototyping.\n",
    "\n",
    "  * All 7 signal classifications\n",
    "  * 1000 simulations / class (7 classes = 7000 files)\n",
    "  * Available as single zip file\n",
    "  * ~2 GB in total\n",
    "\n",
    "### Primary Medium\n",
    "\n",
    "The `primary medium` is a subset of the full primary data set.  Use for prototyping and model building.\n",
    "\n",
    "  * All 7 signal classifications\n",
    "  * 5000 simulations / class (7 classes = 35000 files)\n",
    "  * Large enough for relatively robust model construction\n",
    "  * Available in 5 separate zip files\n",
    "  * ~10 GB in total\n",
    " \n",
    "### Primary Full\n",
    "\n",
    "The `primary full` is the entire primary data set.  Use these data only if you want a very large training data set.\n",
    "\n",
    "  * All 7 signal classifications\n",
    "  * 20000 simulations / class (7 classes = 140000 files)\n",
    "  * Only available in 140k individual files\n",
    "    * one must read through the index file and download files individually, which will take some time from outside of IBM Cloud systems\n",
    "  * ~50 GB in total\n",
    "  \n",
    "\n",
    "## Index Files\n",
    "\n",
    "For all data sets, there exists an **index** file. That file is a CSV file. Each row holds the UUID, signal_classification (label) for a simulation file in the data set. You can use these index files in a few different ways (from using to keep track of your downloads, to facilitate parallelization of your analysis on Spark).   \n",
    "\n",
    "\n",
    "\n",
    "## Direct Data URLs if you are working from outside of IBM Data Science Experience\n",
    "\n",
    "### Basic4\n",
    "[Data (1.1 GB)](https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_basic_v2/basic4.zip)\n",
    "\n",
    "[Index File](https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_files/public_list_basic_v2_26may_2017.csv)\n",
    "\n",
    "\n",
    "### Primary Small\n",
    "\n",
    "[Data (1.9 GB)](https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_v3_zipped/primary_small_v3.zip)\n",
    "\n",
    "[Index File](https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_files/public_list_primary_v3_small_21june_2017.csv)\n",
    "\n",
    "### Primary Medium\n",
    "\n",
    "[Data Zip File 1 (1.9 GB)](https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_v3_zipped/primary_medium_v3_1.zip)\n",
    "\n",
    "[Data Zip File 2 (1.9 GB)](https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_v3_zipped/primary_medium_v3_2.zip)\n",
    "\n",
    "[Data Zip File 3 (1.9 GB)](https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_v3_zipped/primary_medium_v3_3.zip)\n",
    "\n",
    "[Data Zip File 4 (1.9 GB)](https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_v3_zipped/primary_medium_v3_4.zip)\n",
    "\n",
    "[Data Zip File 5 (1.9 GB)](https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_v3_zipped/primary_medium_v3_5.zip)\n",
    "\n",
    "[Index File](https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_files/public_list_primary_v3_medium_21june_2017.csv)\n",
    "\n",
    "\n",
    "It's probably easiest to download these zip files and unzip them. Then move the contents of both to a single folder. (We had to create two separate zip files because there is a 5 GB size limit on Object Storage.)\n",
    "\n",
    "### Primary Full\n",
    "\n",
    "There are two ways to download the full data set. Scroll below to look for a programmatic way. Alternatively, a fellow participant has provided us [the \"ninja\" way to very quickly download the full data set with a single command-line call.](Step_1b_Full_Data_Download.md) \n",
    "\n",
    "## Test Data Set\n",
    "\n",
    "Once you've trained your model, done all of your cross-validation testing, and are ready to submit an entry to the contest, you'll need to download the test data set and score the test set data with your model.  \n",
    "\n",
    "\n",
    "The test data files are nearly the same as the training sets. The only difference is the JSON header in each file does not contain the signal class. You can use `ibmseti` python package to read each file, just like you would the training data. See [Step_2_reading_SETI_code_challenge_data.ipynb](tutorials/Step_2_reading_SETI_code_challenge_data.ipynb) for examples. \n",
    "\n",
    "#### NOTE: July 1 - July 23: This test set is called the \"Preview\" test set.\n",
    "####             July 23 - July 31: The final test set will be available and participants will be notified via Slack and email. \n",
    "\n",
    "<br>\n",
    "\n",
    "The `primary_testset_preview_v3` data set contains 2414 test simulation files. Each data file is the same as the above training data except the JSON header does NOT contain the 'signal_classification' key. \n",
    "\n",
    "  * All 7 classes\n",
    "  * Roughly 340 simulations per class (+- 50) \n",
    "  * JSON header with UUID only\n",
    "  * Available as single zip file\n",
    "  * 665 MB in total\n",
    "\n",
    "**The `primary_testset_final_v3` data set will be available by July 23rd.**\n",
    "\n",
    "### Direct Download Link\n",
    "\n",
    "[Preview Test Set Zip File](https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_v3_zipped/primary_testset_preview_v3.zip)\n",
    "\n",
    "[Preview Test Set Index File](https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_files/public_list_primary_v3_testset_preview.csv)\n",
    "\n",
    "\n",
    "### Submitting Classification Results\n",
    "\n",
    "See the [Judging Criteria](../Judging_Criteria.ipynb) notebook for information on submitting your test-set classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Programmatically Accessing the Data\n",
    "\n",
    "The data are stored in `containers` on IBM Object Storage. You can access these data with HTTP calls. Here we use system level `curl`, but you could easily use the Python `requests` package. \n",
    "\n",
    "The URL for all data files is composed of\n",
    "\n",
    "  `base_url/container/objectname`.\n",
    " \n",
    "The `base_url` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#If you are running this in IBM Apache Spark (via Data Science Experience)\n",
    "base_url = 'https://dal05.objectstorage.service.networklayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b'\n",
    "\n",
    "#ELSE, if you are outside of IBM:\n",
    "base_url = 'https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b'\n",
    "\n",
    "#NOTE: using the 2nd base_url, if you are outside of IBM, will be slower. :/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Defining a local data folder to dump data\n",
    "\n",
    "import os\n",
    "\n",
    "mydatafolder = os.path.join( os.environ['PWD'], 'my_data_folder' )\n",
    "if os.path.exists(mydatafolder) is False:\n",
    "    os.makedirs(mydatafolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Basic Data Set\n",
    "\n",
    "We'll start with the basic data set.  Because the basic data set is small, we've created a `.zip` file of the full data set that you can download directly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "basic_container = 'simsignals_basic_v2'\n",
    "basic4_zip_file = 'basic4.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "os.system('curl {}/{}/{} > {}'.format(base_url, basic_container, basic4_zip_file, mydatafolder + '/' + basic4_zip_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!ls -al my_data_folder/basic4.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Primary Data Set\n",
    "\n",
    "\n",
    "### Primary Small\n",
    "\n",
    "The `primary_small` subset can be found in a zip file in\n",
    "* contianer = 'simignals_v3_zipped'\n",
    "* objectname = 'primary_small.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "filename = 'primary_small_v3.zip'\n",
    "primary_small_url = '{}/simsignals_v3_zipped/{}'.format(base_url, filename)\n",
    "os.system('curl {} > {}'.format(primary_small_url, mydatafolder +'/'+filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### Primary Small Index File\n",
    "\n",
    "A CSV file containing the UUID, signal classifications for each file in the `primary_small` subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "filename = 'public_list_primary_v3_small_21june_2017.csv'\n",
    "primary_small_csv_url = '{}/simsignals_files/{}'.format(base_url, filename)\n",
    "os.system('curl {} > {}'.format(primary_small_csv_url, mydatafolder +'/'+filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Primary Medium\n",
    "\n",
    "Similarly, the `primary_medium` subset can be found in a handful of zip files\n",
    "\n",
    "* contianer = 'simignals_v2_zipped'\n",
    "* objectname = 'primary_medium_N.zip'\n",
    "* for N = 1, 2, 3, 4, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "med_N = '{}/simsignals_v3_zipped/primary_medium_v3_{}.zip'\n",
    "\n",
    "for i in range(1,6):\n",
    "    med_url = med_N.format(base_url, i)\n",
    "    output_file = mydatafolder + '/primary_medium_v3_{}.zip'.format(i)\n",
    "    print 'GETing', output_file\n",
    "    os.system('curl {} > {}'.format(med_url, output_file ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### Primary Medium Index File\n",
    "\n",
    "Here too, there is a CSV file containing the UUID, signal classifications for each file in the `primary_medium` subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "filename = 'public_list_primary_v3_medium_21june_2017.csv'\n",
    "med_csv_url = '{}/simsignals_files/{}'.format(base_url, filename)\n",
    "os.system('curl {} > {}'.format(med_csv_url, mydatafolder +'/'+filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Primary Full set\n",
    "\n",
    "Because the full set is so incredibly large, we only have these 140,000 files available individually on object storage. \n",
    "\n",
    "A fellow participant has provided us [a nice way to very quickly download the full data set with a single command-line call.](Step_1b_Full_Data_Download.md) \n",
    "\n",
    "Or you can follow the instructions below.\n",
    "\n",
    "The `primary_full` list can be found here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "filename = 'public_list_primary_v3_full_21june_2017.csv'\n",
    "prim_full = '{}/simsignals_files/{}'.format(base_url, filename)\n",
    "os.system('curl {} > {}'.format(prim_full, mydatafolder +'/'+filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "One can download this list and begin to pull down files individually if desired. Warning, **however, this will take approximately a billion years if you are not running on IBM Apache Spark** -- IBM Apache Spark and Object Storage exist in the same data center and share a fast network connection. \n",
    "\n",
    "The data are found in \n",
    "\n",
    "`base_url/simsignals_v2/<uuid>.dat`\n",
    "\n",
    "For example:\n",
    "\n",
    "https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_v3/03fbb014-344a-42bc-a3e4-f755557a3628.dat\n",
    "\n",
    "**We are working to make the primary full data set more easily, as this current setup is less than ideal. You will be notified if that becomes available. The data will be directly available for participants of the hackathon, however.**\n",
    "\n",
    "If you wish to programmatically begin to download the full data set you may use the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "file_list_container = 'simsignals_files'\n",
    "file_list = 'public_list_primary_v3_full_21june_2017.csv'\n",
    "primary_data_container = 'simsignals_v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "r = requests.get('{}/{}/{}'.format(base_url, file_list_container, file_list), timeout=(9.0, 21.0))\n",
    "filecontents = copy.copy(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_primary_files = [line.split(',') for line in filecontents.split('\\n')]\n",
    "full_primary_files = full_primary_files[1:-1] #strip the header and empty last element\n",
    "full_primary_files = map(lambda x: x[0]+\".dat\", full_primary_files)  #now list of file names (<uuid>.dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#save your data into a local subfolder\n",
    "save_to_folder = mydatafolder + '/primary_data_set'\n",
    "if os.path.exists(save_to_folder) is False:\n",
    "    os.mkdir(save_to_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "total = len(full_primary_files)\n",
    "for row in full_primary_files:\n",
    "    r = requests.get('{}/{}/{}'.format(base_url, primary_data_container, row), timeout=(9.0, 21.0))\n",
    "    \n",
    "    if count % 100 == 0:\n",
    "        print 'done ', count, ' out of ',  total\n",
    "    count += 1\n",
    "    \n",
    "    with open('{}/{}'.format(save_to_folder, row), 'w' ) as fout:\n",
    "        fout.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### This is really a lot of data\n",
    "\n",
    "This will be a difficult data set to consume and process if you are using free-tier levels of software from any Cloud provider. You will likely want to have a robust machine, or sets of machines, with many threads and GPUs if you want to train models with such a large dat set. \n",
    "\n",
    "For example, if you have access to an IBM Spark Enterprise cluster, because the network connection between IBM Spark and IBM Object Storage is so fast, we recommend that you **do NOT** download each file. Instead you could parallelize the index file and then retrieve and process each file on a worker node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Using Spark -- can parallelize the job across your worker nodes\n",
    "import ibmseti\n",
    "def retrieve_and_process(row):\n",
    "    try:\n",
    "        r = requests.get('{}/{}/{}'.format(base_url, primary_data_container, row), timeout=(9.0, 21.0))\n",
    "    except Exception as e:\n",
    "        return (row, 'failed', [])\n",
    "    \n",
    "    aca = ibmseti.compamp.SimCompamp(r.content)\n",
    "    spectrogram = aca.get_spectrogram() # or do something else\n",
    "    features = my_feature_extractor(spectrogram) #example external function for reducing the spectrogram into a handful of features, perhaps\n",
    "    \n",
    "    signal_class = aca.header()['signal_classifiation']\n",
    "        \n",
    "    return (row, signal_class, features)\n",
    "\n",
    "npartitions = 60  \n",
    "rdd = sc.parallelize(full_primary_files, npartitions)\n",
    "\n",
    "#Now ask Spark to run the job\n",
    "process_results = rdd.map(retrieve_and_process).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
